{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport math\n\nimport time\nimport datetime\n\nimport os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n\nfrom transformers import AutoTokenizer, TFAutoModel, AutoConfig, TFBertModel\nimport tensorflow as tf\n\nimport pickle\n\nfrom sklearn.model_selection import KFold\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nimport math\n\nfrom datetime import datetime\nimport string\nimport warnings\n\n\nwarnings.filterwarnings('ignore')","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":8.946182,"end_time":"2021-05-17T05:53:18.097137","exception":false,"start_time":"2021-05-17T05:53:09.150955","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-05-21T19:34:47.58547Z","iopub.execute_input":"2021-05-21T19:34:47.586055Z","iopub.status.idle":"2021-05-21T19:34:56.376217Z","shell.execute_reply.started":"2021-05-21T19:34:47.585969Z","shell.execute_reply":"2021-05-21T19:34:56.375112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n    strategy = tf.distribute.get_strategy()\n\nAUTO = tf.data.experimental.AUTOTUNE\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","metadata":{"papermill":{"duration":5.794272,"end_time":"2021-05-17T05:53:23.907273","exception":false,"start_time":"2021-05-17T05:53:18.113001","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-05-21T19:34:56.377811Z","iopub.execute_input":"2021-05-21T19:34:56.378125Z","iopub.status.idle":"2021-05-21T19:35:02.177108Z","shell.execute_reply.started":"2021-05-21T19:34:56.378094Z","shell.execute_reply":"2021-05-21T19:35:02.176048Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"url_test = \"https://raw.githubusercontent.com/ipavlopoulos/toxic_spans/master/data/tsd_test.csv\"\nurl_train = \"https://raw.githubusercontent.com/ipavlopoulos/toxic_spans/master/data/tsd_train.csv\"\nurl_trial = \"https://raw.githubusercontent.com/ipavlopoulos/toxic_spans/master/data/tsd_trial.csv\"\n\ntrain_df = pd.read_csv(url_train, error_bad_lines=False)\ntest_df = pd.read_csv(url_test, error_bad_lines=False)\ntrial_df = pd.read_csv(url_trial, error_bad_lines=False)","metadata":{"papermill":{"duration":1.239946,"end_time":"2021-05-17T05:53:25.164133","exception":false,"start_time":"2021-05-17T05:53:23.924187","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-05-21T19:35:02.179Z","iopub.execute_input":"2021-05-21T19:35:02.179319Z","iopub.status.idle":"2021-05-21T19:35:02.646124Z","shell.execute_reply.started":"2021-05-21T19:35:02.179287Z","shell.execute_reply":"2021-05-21T19:35:02.645374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing","metadata":{"papermill":{"duration":0.015965,"end_time":"2021-05-17T05:53:25.196542","exception":false,"start_time":"2021-05-17T05:53:25.180577","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import numpy as np\n\nsynonyms = ['calumniation', 'insult', 'swearing', 'threat', 'discrimination', \n            'toxic words', 'severe toxic words', 'poisonous words', \n            'severe poisonous words', 'hate speech', 'offensive language', \n            'hatred', 'anger', 'violence', 'abuse', 'rudeness', \n            'profanity', 'cursing', 'intimidation', 'bullying', \n            'oppression', 'menace', 'stereotype', 'sexual harassment', 'hateful words'] \n\n\ndicriminations = ['adultism', 'ageism', 'age discrimination',\n                  'caste,ableism', 'disablism', 'disability discrimination', \n                  'linguistic discrimination', 'racism', 'racial discrimination', \n                  'discrimination based on skin colour', 'ethnic discrimination', \n                  'racial segregation', 'religious bigotry', 'religious discrimination',\n                  'sexism', 'homophobia', 'misogyny', 'misandry', 'transphobia', \n                  'biphobia', 'lookism', 'antisemitism', 'hispanophobia', \n                  'islamophobia', 'sizeism', 'xenophobia', 'chauvinism', \n                  'afrophobia', 'anti-arabism', 'apostasy', 'colourism', \n                  'heightism', 'discrimination against intersex people', \n                  'supremacism', 'genetic discrimination', 'mentalism', \n                  'antisexualism', 'anti-Catholicism'] \n\npatterns = ['Does the text include toxicity, such as', \n            'Does the text include',\n            'Select spans of toxicity in the text, such as', \n            'Find spans of toxicity in the text, such as',\n            'Find in the text spans of toxicity, such as']","metadata":{"execution":{"iopub.status.busy":"2021-05-21T19:35:02.647646Z","iopub.execute_input":"2021-05-21T19:35:02.648161Z","iopub.status.idle":"2021-05-21T19:35:02.655062Z","shell.execute_reply.started":"2021-05-21T19:35:02.648128Z","shell.execute_reply":"2021-05-21T19:35:02.654232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_question(number_words):\n    \n    pattern = np.random.choice(patterns, 1)[0]\n    flag = 'Does' in pattern\n    \n    toxic_words = set()\n\n    while len(toxic_words) < number_words:\n        if np.random.random() > 0.5:\n            toxic_words.add(np.random.choice(synonyms, 1)[0])\n        else:\n            toxic_words.add(np.random.choice(dicriminations, 1)[0])\n            \n    question = pattern + ' ' + ', '.join(toxic_words) + '?'* flag\n    \n    return question","metadata":{"execution":{"iopub.status.busy":"2021-05-21T19:35:02.656141Z","iopub.execute_input":"2021-05-21T19:35:02.656547Z","iopub.status.idle":"2021-05-21T19:35:02.672766Z","shell.execute_reply.started":"2021-05-21T19:35:02.656519Z","shell.execute_reply":"2021-05-21T19:35:02.671861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class NERTokenizerWords:\n    \n    def __init__(self, df, question, model_name, MAX_LEN, num_words = 25):\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name, do_lower_case=True, use_fast = False)\n        self.model_name = model_name\n        self.MAX_LEN = MAX_LEN\n        \n        \n        self.contexts, self.answers = self.get_contexts_answers(df)\n        self.shape = len(self.contexts)\n        \n        if question == None:\n            np.random.seed(SEED)\n            self.questions = [get_question(num_words)  for _ in range(self.shape)]\n        else:\n            self.questions = [question] * self.shape\n        \n    def get_contexts_answers(self, df):\n        contexts = []\n        answers = []\n\n        num_imp = 0\n\n        for i in range(len(df)):\n\n            text_str = df['spans'][i]\n            splitted_str = text_str[1:-1].split(\", \")\n            context = df['text'][i]\n            splitted_context = context.split()\n\n            if len(splitted_str) == 1:\n                contexts.append(context)\n                answers.append([0]*len(splitted_context))\n                num_imp += 1\n                continue\n\n\n            splitted_str = list(map(int, splitted_str))\n\n\n            tags = [0]*len(splitted_context)\n            char_tags = [0]*len(context)\n\n\n            offsets = []; idx = 0;\n            for word in splitted_context:\n                idx = context.find(word, idx)\n                offsets.append((idx, idx + len(word) + 1))\n                idx += len(word)\n\n\n            for pos in splitted_str:\n                char_tags[pos] = 1\n\n            for word_num, offset in enumerate(offsets):\n                start_word, end_word = offset\n                if sum(char_tags[start_word:end_word]) > 0:\n                    tags[word_num] = 1\n\n\n            contexts.append(' '.join(splitted_context)) \n            answers.append(tags)\n            \n        print(\"Number of impossible: \", num_imp)\n\n        return contexts, answers\n\n    def create_inputs_targets(self):\n        \n        roberta_flag = self.model_name == 'roberta-base' or self.model_name == 'roberta-large'\n        \n        dataset_dict = {\n                \"input_ids\": [],\n                \"token_type_ids\": [],\n                \"attention_mask\": [],\n                \"tags\": []\n            }\n\n        input_ids = []\n        target_tags = []\n        num_dropped = 0\n\n        for context, question, answer in zip(self.contexts, self.questions, self.answers):\n            input_ids = []\n            target_tags = []\n            for idx, word in enumerate(context.split()):\n                if roberta_flag:\n                    word = ' ' + word\n                ids = self.tokenizer.encode(word, add_special_tokens=False)\n                input_ids.extend(ids)\n\n                tokenized_words = self.tokenizer.tokenize(word)\n                for tokenized_word in tokenized_words:\n                    if tokenized_word in ',.!?':\n                        target_tags.extend([0])\n                    else:\n                        target_tags.extend([answer[idx]])\n\n            enc_question = self.tokenizer.encode(question, add_special_tokens=False)\n            question_tags = [0]*len(enc_question)\n            \n            if roberta_flag:\n                sep_tokens = 2\n            else:\n                sep_tokens = 1\n                \n            token_type_ids = [0] + question_tags + [0]*sep_tokens + [1] * len(input_ids) + [0]\n            input_ids = [self.tokenizer.cls_token_id] + enc_question + [self.tokenizer.sep_token_id]*sep_tokens + input_ids + [self.tokenizer.sep_token_id]\n            target_tags = [0] + question_tags + [0]*sep_tokens + target_tags + [0]\n            attention_mask = [1] * len(input_ids)\n            padding_len = self.MAX_LEN - len(input_ids)\n\n            if padding_len < 0:\n                num_dropped += 1\n                continue\n\n            input_ids = input_ids + ([self.tokenizer.pad_token_id] * padding_len)\n            attention_mask = attention_mask + ([self.tokenizer.pad_token_id] * padding_len)\n            token_type_ids = token_type_ids + ([self.tokenizer.pad_token_id] * padding_len)\n            target_tags = target_tags + ([2] * padding_len)\n\n\n            dataset_dict[\"input_ids\"].append(input_ids)\n            dataset_dict[\"token_type_ids\"].append(token_type_ids)\n            dataset_dict[\"attention_mask\"].append(attention_mask)\n            dataset_dict[\"tags\"].append(target_tags)\n\n\n        for key in dataset_dict:\n            dataset_dict[key] = np.array(dataset_dict[key])\n\n        x = [\n            dataset_dict[\"input_ids\"],\n            dataset_dict[\"token_type_ids\"],\n            dataset_dict[\"attention_mask\"],\n        ]\n        y = dataset_dict[\"tags\"]\n\n        return x, y ","metadata":{"papermill":{"duration":0.044253,"end_time":"2021-05-17T05:53:25.257054","exception":false,"start_time":"2021-05-17T05:53:25.212801","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-05-21T19:35:02.673974Z","iopub.execute_input":"2021-05-21T19:35:02.674467Z","iopub.status.idle":"2021-05-21T19:35:02.703502Z","shell.execute_reply.started":"2021-05-21T19:35:02.674424Z","shell.execute_reply":"2021-05-21T19:35:02.702094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{"papermill":{"duration":0.016053,"end_time":"2021-05-17T05:53:25.289555","exception":false,"start_time":"2021-05-17T05:53:25.273502","status":"completed"},"tags":[]}},{"cell_type":"code","source":" loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False, reduction=tf.keras.losses.Reduction.NONE)\n\ndef masked_ce_loss(real, pred):\n    mask = tf.math.logical_not(tf.math.equal(real, 2))\n    loss_ = loss_object(real, pred)\n\n    mask = tf.cast(mask, dtype=loss_.dtype)\n    loss_ *= mask\n\n    return tf.reduce_mean(loss_)\n\ndef create_model():\n    ## BERT encoder\n    config = AutoConfig.from_pretrained(model_name)\n    encoder = TFAutoModel.from_pretrained(model_name, config = config)\n    #alencoder = TFBertModel.from_pretrained(model_name, config = config, from_pt = True)\n\n    ## NER Model\n    input_ids = tf.keras.layers.Input(shape=(MAX_LEN,), dtype=tf.int32)\n    token_type_ids = tf.keras.layers.Input(shape=(MAX_LEN,), dtype=tf.int32)\n    attention_mask = tf.keras.layers.Input(shape=(MAX_LEN,), dtype=tf.int32)\n    embedding = encoder(input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)[0]\n    \n    #embedding = tf.keras.layers.Dropout()(embedding)\n    #tag_logits = tf.keras.layers.Dense(num_tags+1, activation='softmax')(embedding)\n    \n    embedding = tf.keras.layers.Dropout(Dropout_new)(embedding)\n    embedding = tf.keras.layers.Conv1D(768, 2, padding='same')(embedding)\n    embedding = tf.keras.layers.LeakyReLU()(embedding)\n    embedding = tf.keras.layers.Conv1D(64, 2,padding='same')(embedding)\n#     embedding = tf.keras.layers.Dense(1)(embedding)\n#     embedding = tf.keras.layers.Flatten()(embedding)\n    tag_logits = tf.keras.layers.Dense(num_tags + 1, activation = 'softmax')(embedding)\n    \n    model = tf.keras.models.Model(\n        inputs=[input_ids, token_type_ids, attention_mask],\n        outputs=[tag_logits],\n    )\n    optimizer = tf.keras.optimizers.Adam(lr=lr)\n    model.compile(optimizer=optimizer, loss=masked_ce_loss, metrics=['accuracy'])\n    return model","metadata":{"papermill":{"duration":0.031753,"end_time":"2021-05-17T05:53:25.337616","exception":false,"start_time":"2021-05-17T05:53:25.305863","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-05-21T19:35:02.705168Z","iopub.execute_input":"2021-05-21T19:35:02.705603Z","iopub.status.idle":"2021-05-21T19:35:02.722022Z","shell.execute_reply.started":"2021-05-21T19:35:02.705564Z","shell.execute_reply":"2021-05-21T19:35:02.721303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def jaccard_score(pred, true): \n    pred_tokens = set(pred.lower().split())\n    true_tokens = set(true.lower().split())\n    if (len(pred_tokens)==0) & (len(true_tokens)==0): \n        return 0.5\n    inter_tokens = pred_tokens.intersection(true_tokens)\n    return float(len(inter_tokens)) / (len(pred_tokens) + len(true_tokens) - len(inter_tokens))\n\n\ndef f1_score(pred, true):\n    pred_tokens = pred.lower().split()\n    true_tokens = true.lower().split()\n\n    if len(pred_tokens) == 0 or len(true_tokens) == 0:\n        return int(pred_tokens == true_tokens) \n\n    common_tokens = set(pred_tokens) & set(true_tokens)\n\n    if len(common_tokens) == 0:\n        return 0\n\n    prec = len(common_tokens) / len(pred_tokens)\n    rec = len(common_tokens) / len(true_tokens)\n\n    return 2 * (prec * rec) / (prec + rec)","metadata":{"papermill":{"duration":0.028615,"end_time":"2021-05-17T05:53:25.382848","exception":false,"start_time":"2021-05-17T05:53:25.354233","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-05-21T19:35:02.724069Z","iopub.execute_input":"2021-05-21T19:35:02.724516Z","iopub.status.idle":"2021-05-21T19:35:02.737759Z","shell.execute_reply.started":"2021-05-21T19:35:02.724484Z","shell.execute_reply":"2021-05-21T19:35:02.736936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict(x, y_pred, y_real, prefix = 'TRAIN'):\n    f1_scores, jaccard_scores = [], []\n    predictions, true_answers = [], []\n    shape = y_real.shape[0]\n    input_ids = x[0]\n    for i in range(shape):\n        sep_token = np.where(input_ids[i] == tokenizer.sep_token_id)[0][-1]\n        pred_tokens = np.where(y_pred[i] == 1)[0]\n        pred_tokens = pred_tokens[pred_tokens < sep_token]\n        pred_tokens = input_ids[i][pred_tokens]\n        \n        real_tokens = input_ids[i][np.where(y_real[i] == 1)[0]]\n        \n        pred_words = tokenizer.decode(pred_tokens)\n        real_words = tokenizer.decode(real_tokens)\n        \n        predictions.append(pred_words)\n        true_answers.append(real_words)\n        \n        jaccard_sc = jaccard_score(pred_words, real_words)\n        f1_sc = f1_score(pred_words, real_words)\n        \n        f1_scores.append(f1_sc)\n        jaccard_scores.append(jaccard_sc)\n    \n    print(f'{prefix} MEAN F1-SCORE {np.round(np.mean(f1_scores), 3)}')\n    print(f'{prefix} MEAN JAC-SCORE {np.round(np.mean(jaccard_scores), 3)}')\n    \n    pred_csv = pd.DataFrame({'predictions': predictions, 'true_answers': true_answers, 'f1_scores': f1_scores, 'jaccard_scores': jaccard_scores})\n    pred_csv.to_csv(f\"{prefix}_{model_name.replace('/', '_')}_predictions.csv\", index = False)\n        \n    return predictions, true_answers, f1_scores, jaccard_scores","metadata":{"papermill":{"duration":0.030891,"end_time":"2021-05-17T05:53:25.430718","exception":false,"start_time":"2021-05-17T05:53:25.399827","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-05-21T19:35:02.739238Z","iopub.execute_input":"2021-05-21T19:35:02.739674Z","iopub.status.idle":"2021-05-21T19:35:02.755001Z","shell.execute_reply.started":"2021-05-21T19:35:02.739643Z","shell.execute_reply":"2021-05-21T19:35:02.754216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{"papermill":{"duration":0.016792,"end_time":"2021-05-17T05:53:25.464761","exception":false,"start_time":"2021-05-17T05:53:25.447969","status":"completed"},"tags":[]}},{"cell_type":"code","source":"MAX_LEN = 256\nmodel_name = 'albert-xxlarge-v2'\nnum_tags = 2\n\nEPOCHS = 10 # originally 3\nSEED = 88888\n\nLABEL_SMOOTHING = 0.1\ntf.random.set_seed(SEED)\nnp.random.seed(SEED)\nAUTO = tf.data.experimental.AUTOTUNE\n\nDropout_new = 0.3 # originally 0.1\nn_split = 2         # originally 5\nlr = 5e-5           # originally 3e-5\nnum_words = 20","metadata":{"papermill":{"duration":0.025813,"end_time":"2021-05-17T05:53:25.507733","exception":false,"start_time":"2021-05-17T05:53:25.48192","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-05-21T19:35:02.756125Z","iopub.execute_input":"2021-05-21T19:35:02.756543Z","iopub.status.idle":"2021-05-21T19:35:02.774371Z","shell.execute_reply.started":"2021-05-21T19:35:02.756513Z","shell.execute_reply":"2021-05-21T19:35:02.773306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"use_tpu = None\nexp = True\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    use_tpu = True\nexcept:\n    use_tpu = False\n\nif use_tpu:\n    # Create distribution strategy\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n\n    # Create model\n    with strategy.scope():\n        model = create_model()\nelse:\n    model = create_model()\n    \n\ncopy_train_df = train_df\n\nquestion = \"Does the text include calumniation, insult, swearing, threat, discrimination, toxic or severe toxic words?\"\n\nif exp:\n    train_questions, val_questions, test_questions = None, None, None\nelse:\n    train_questions, val_questions, test_questions = question, question, question\n    \ntrain_tokenized = NERTokenizerWords(copy_train_df, train_questions, model_name, MAX_LEN, num_words)\ntokenizer = train_tokenized.tokenizer\nx_train, y_train = train_tokenized.create_inputs_targets()\n\n\nval_tokenized = NERTokenizerWords(trial_df, val_questions, model_name, MAX_LEN, num_words)\nx_val, y_val = val_tokenized.create_inputs_targets()\n\ntest_tokenized = NERTokenizerWords(test_df, test_questions, model_name, MAX_LEN, num_words)\nx_test, y_test = test_tokenized.create_inputs_targets()\n\nBATCH_SIZE = 256 if tpu else 16\n\nmodel.fit(\n    x_train,\n    y_train,\n    epochs = EPOCHS,\n    verbose = 1,\n    batch_size = BATCH_SIZE,\n    validation_split = 0.1\n)\n    \n\ntrain_predictions = model.predict(x_train, verbose = True)\ntrain_pred_tags = np.argmax(train_predictions, 2)\n\nval_predictions = model.predict(x_val, verbose = True)\nval_pred_tags = np.argmax(val_predictions, 2)\n\ntrain_predictions, train_true_values, tr_f1_scores, tr_jaccard_scores = predict(x_train, train_pred_tags, y_train, prefix='train')\nval_predictions, val_true_values, val_f1_scores, val_jaccard_scores = predict(x_val, val_pred_tags, y_val, prefix='val')\n\ntest_predictions = model.predict(x_test, verbose = True)\ntest_pred_tags = np.argmax(test_predictions, 2)\n\ntest_predictions, test_true_values, test_f1_scores, test_jaccard_scores = predict(x_test, test_pred_tags, y_test, prefix='test')\n\nmodel.fit(\n    x_val,\n    y_val,\n    epochs = EPOCHS // 3,\n    verbose = 1,\n    batch_size = BATCH_SIZE,\n    validation_split = 0.1\n)\n\n\ntest_predictions = model.predict(x_test, verbose = True)\ntest_pred_tags = np.argmax(test_predictions, 2)\n\ntest_predictions, test_true_values, test_f1_scores, test_jaccard_scores = predict(x_test, test_pred_tags, y_test, prefix='TEST')\n\nprint(f'{model_name} finished')","metadata":{"papermill":{"duration":301.070791,"end_time":"2021-05-17T05:58:26.59563","exception":false,"start_time":"2021-05-17T05:53:25.524839","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-05-21T19:35:02.775982Z","iopub.execute_input":"2021-05-21T19:35:02.776426Z","iopub.status.idle":"2021-05-21T19:57:18.927859Z","shell.execute_reply.started":"2021-05-21T19:35:02.776372Z","shell.execute_reply":"2021-05-21T19:57:18.926768Z"},"trusted":true},"execution_count":null,"outputs":[]}]}